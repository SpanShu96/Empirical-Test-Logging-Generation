{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QU7f6CkNqEdZ"},"outputs":[],"source":["from IPython.display import clear_output\n","\n","# %env USE_AUTH_EPHEM=0\n","!pip install -q t5\n","!pip install -U jax jaxlib\n","!pip install -U flax\n","\n","clear_output()"]},{"cell_type":"code","source":["from IPython.display import clear_output\n","\n","!pip install -U tensorflow-gcs-config==2.12.0\n","!pip install tensorflow==2.12.0\n","!pip install tensorflow-text==2.12.0\n","# !pip install -U jax jaxlib\n","\n","clear_output()"],"metadata":{"id":"STFUuETPOvCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2h1MRzBLtex2"},"outputs":[],"source":["# %tensorflow_version 2.x\n","# !pip3 install --upgrade pip\n","# !pip install t5==0.9.2\n","# !pip install -U jax jaxlib\n","\n","%env USE_AUTH_EPHEM=0\n","import functools\n","import os\n","import gin\n","import tensorflow_gcs_config\n","from google.colab import auth\n","import tensorflow.compat.v1 as tf\n","import tensorflow_datasets as tfds\n","from contextlib import contextmanager\n","import logging as py_logging\n","import t5\n","tf.app.flags.DEFINE_string('f', '', 'kernel')\n","\n","#Set the base dir(Google cloud bucket)\n","BASE_DIR = \"\" #@param { type: \"string\" }\n","\n","\n","# Set credentials for GCS reading/writing from Colab and TPU.\n","TPU_TOPOLOGY = \"2x2\"\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","  TPU_ADDRESS = tpu.get_master()\n","  print('Running on TPU:', TPU_ADDRESS)\n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","auth.authenticate_user()\n","# auth.authenticate_service_account()\n","tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","tensorflow_gcs_config.configure_gcs_from_colab_auth()\n","\n","tf.disable_v2_behavior()\n","\n","\n","#LOGGING\n","tf.get_logger().propagate = False\n","py_logging.root.setLevel('INFO')\n","\n","@contextmanager\n","def tf_verbosity_level(level):\n","  og_level = tf.logging.get_verbosity()\n","  tf.logging.set_verbosity(level)\n","  yield\n","  tf.logging.set_verbosity(og_level)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyI3CXuOpbX2"},"outputs":[],"source":["from t5.data import postprocessors as t5_postprocessors\n","from t5.seqio import Feature,SentencePieceVocabulary\n","\n","\n","vocab_model_path = '' #@param { type: \"string\" }\n","vocab_path = '' #@param { type: \"string\" }\n","\n","\n","TaskRegistry = t5.data.TaskRegistry\n","TfdsTask = t5.data.TfdsTask\n","\n","\n","def get_default_vocabulary():\n","  return SentencePieceVocabulary(vocab_model_path, 100)\n","\n","DEFAULT_OUTPUT_FEATURES = {\n","    \"inputs\": Feature(\n","        vocabulary=get_default_vocabulary(), add_eos=True, required=False),\n","\n","    \"targets\": Feature(\n","        vocabulary=get_default_vocabulary(), add_eos=True)\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glLJUm1dxIiH"},"outputs":[],"source":["#Skip this cell for running the pre-training on the second task only\n","\n","#6755884\n","path_pretraining_task1 = ''#@param { type: \"string\" }\n","\n","nq_tsv_path = {\n","    \"train\":      path_pretraining_task1,\n","}\n","\n","num_nq_examples_task1 = dict(train=6755884)\n","\n","def nq_dataset_task1(split, shuffle_files=True):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","\n","  ds = tf.data.TextLineDataset(nq_tsv_path[split])\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n","                        field_delim=\"\\t\", use_quote_delim=True),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n","  return ds\n","\n","print(\"A few raw train examples...\")\n","for ex in tfds.as_numpy(nq_dataset_task1(\"train\").take(5)):\n","    print(ex)\n","\n","\n","def preprocessing_task1(ds):\n","\n","  def to_inputs_and_targets(ex):\n","\n","        inputs = tf.strings.join(['DENOISE: ' + ex['input']], separator=' ')\n","        class_label = tf.strings.join([ex['output']], separator=' ')\n","        return {'inputs': inputs, 'targets': class_label }\n","\n","  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","t5.data.TaskRegistry.remove('masking_task')\n","t5.data.TaskRegistry.add(\n","    \"masking_task\",\n","    dataset_fn=nq_dataset_task1,\n","    splits=[\"train\"],\n","    text_preprocessor=preprocessing_task1,\n","    output_features = DEFAULT_OUTPUT_FEATURES,\n","    num_input_examples=num_nq_examples_task1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGeC8sNEqFfp"},"outputs":[],"source":["#Skip this cell for running the pre-training on the first task only\n","\n","#133082\n","path_pretraining_task2 = ''#@param { type: \"string\" }\n","\n","nq_tsv_path = {\n","    \"train\":      path_pretraining_task2,\n","\n","}\n","num_nq_examples_task2 = dict(train=133082)\n","\n","def nq_dataset_task2(split, shuffle_files=True):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","\n","  ds = tf.data.TextLineDataset(nq_tsv_path[split])\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n","                        field_delim=\"\\t\", use_quote_delim=True),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n","  return ds\n","\n","print(\"A few raw train examples...\")\n","for ex in tfds.as_numpy(nq_dataset_task1(\"train\").take(5)):\n","    print(ex)\n","\n","\n","def preprocessing_task2(ds):\n","\n","      def to_inputs_and_targets(ex):\n","\n","        inputs = tf.strings.join(['LOG_STMT: ' + ex['input']], separator=' ')\n","        class_label = tf.strings.join([ex['output']], separator=' ')\n","        return {'inputs': inputs, 'targets': class_label }\n","\n","\n","      return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","t5.data.TaskRegistry.remove('log_stmt_task')\n","t5.data.TaskRegistry.add(\n","    \"log_stmt_task\",\n","    dataset_fn=nq_dataset_task2,\n","    splits=[\"train\"],\n","    text_preprocessor=preprocessing_task2,\n","    output_features = DEFAULT_OUTPUT_FEATURES,\n","    num_input_examples=num_nq_examples_task2\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686018501349,"user":{"displayName":"Honglin Shu","userId":"14794632410771969422"},"user_tz":-540},"id":"oDEHaEz9uP5z","outputId":"e9b9e67a-97e4-451b-c5bd-7474bbcd9e8e"},"outputs":[{"data":{"text/plain":["<seqio.dataset_providers.Mixture at 0x7f6bc009a650>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["def _rate_num_input_examples(task):\n","  if \"train\" in task.splits:\n","    return float(task.num_input_examples(\"train\"))\n","  elif \"validation\" in task.splits:\n","    return float(task.num_input_examples(\"validation\"))\n","  else:\n","    raise ValueError(\"Task %s does not have a train or validation split.\" % (task.name))\n","\n","\n","### Adjsut the mixture according, to the selected experiment\n","\n","#For denoising only task\n","# t5.data.MixtureRegistry.remove(\"pretraining\")\n","# t5.data.MixtureRegistry.add(\n","#     \"pretraining\",\n","#     [\"masking_task\"],\n","#     default_rate=_rate_num_input_examples\n","# )\n","\n","\n","#For log stmt only task\n","# t5.data.MixtureRegistry.remove(\"pretraining\")\n","# t5.data.MixtureRegistry.add(\n","#     \"pretraining\",\n","#     [\"log_stmt_task\"],\n","#     default_rate=_rate_num_input_examples\n","# )\n","\n","#MT mixture\n","t5.data.MixtureRegistry.remove(\"pretraining\")\n","t5.data.MixtureRegistry.add(\n","    \"pretraining\",\n","    [\"masking_task\",\"log_stmt_task\"],\n","    default_rate=_rate_num_input_examples\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3Qx699vN302"},"outputs":[],"source":["from mesh_tensorflow.transformer.learning_rate_schedules import learning_rate_schedule_noam\n","from t5 import models\n","\n","\n","MODEL_SIZE = \"small\"\n","\n","MODEL_DIR = ''#@param { type: \"string\" }\n","\n","model_parallelism, train_batch_size, keep_checkpoint_max = {\n","    \"small\": (1, 128, 16),\n","    \"base\": (2, 16, 8),\n","    \"large\": (8, 64, 4),\n","    \"3B\": (8, 16, 1),\n","    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n","\n","tf.io.gfile.makedirs(MODEL_DIR)\n","\n","ON_CLOUD = True\n","model = models.mtf_model.MtfModel(\n","    model_dir=MODEL_DIR,\n","    tpu=TPU_ADDRESS,\n","    tpu_topology=TPU_TOPOLOGY,\n","    model_parallelism=model_parallelism,\n","    batch_size=train_batch_size,\n","    learning_rate_schedule = learning_rate_schedule_noam,\n","    sequence_length={\"inputs\": 512, \"targets\": 512},\n","    save_checkpoints_steps=5000,\n","    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n","    iterations_per_loop=100,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6oHp5ScE7nf2"},"outputs":[],"source":["PATH_GIN_FILE = ''#@param { type: \"string\" }\n","import gin\n","\n","with gin.unlock_config():\n","    gin.parse_config_file(PATH_GIN_FILE)\n","    #RUN FINE-TUNING\n","    TRAIN_STEPS = 250000\n","    model.train(\"pretraining\", TRAIN_STEPS)\n"]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}