{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baeca1b1-414f-475e-ada3-aca0375d7922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import random\n",
    "import jsonlines\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from trl import SFTTrainer\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc97bb-3dd9-4cb6-affa-2de620462683",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbf06c1-c962-4610-9837-ce9c99a40e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogGenDataset(Dataset):\n",
    "    def __init__(self, tsv_dataset):\n",
    "        '''tsv dataset contain two columns. \n",
    "        First colomn includes the input.\n",
    "        Second column includes the target.'''\n",
    "\n",
    "        self.dataset = pd.read_csv(tsv_dataset, sep='\\t', header=None)\n",
    "\n",
    "        self.samples = []\n",
    "        for input, target in zip(self.dataset[0].tolist(), self.dataset[1].tolist()):\n",
    "            self.samples.append((input, target))\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.dataset[1].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[1].tolist())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.samples[idx][0]\n",
    "        target = self.samples[idx][1]\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2de990-76f0-49fb-b113-a85694f51edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = './Data/train_log4j.tsv'\n",
    "eval_df_path = './Data/eval_log4j.tsv'\n",
    "test_df_path = './Data/test_log4j.tsv'\n",
    "\n",
    "train_set = LogGenDataset(train_df_path)\n",
    "eval_set = LogGenDataset(eval_df_path)\n",
    "test_set = LogGenDataset(test_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588de127-943a-417d-b076-ee64b253075b",
   "metadata": {},
   "source": [
    "# Build Instruction-tuning Dataset for Code Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f793ed6-6b34-4047-ab99-eae368388c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "T1 = time.perf_counter()\n",
    "\n",
    "# preprocessing \n",
    "train_message_list = []\n",
    "for idx in tqdm(range(len(train_set))):\n",
    "    input = train_set[idx][0]\n",
    "    target = train_set[idx][1]\n",
    "    prompt = \"You are a logging statement generator for Java. \" \\\n",
    "             \"You will be provided with a Java method as input. \" \\\n",
    "             \"Your task is to inject at least one logging statement at a rational position. \" \\\n",
    "             \"The output must be a completed Java method.\"\n",
    "    query = input \n",
    "    label = target\n",
    "    messages = {\"messages\": [{\"role\": \"system\", \"content\": prompt}, \n",
    "                             {\"role\": \"user\", \"content\": query}, \n",
    "                             {\"role\": \"assistant\", \"content\": label}]}\n",
    "\n",
    "    train_message_list.append(messages)\n",
    "\n",
    "with open('./Data/train_data_codellama_it.jsonl', 'w') as f:\n",
    "    for m in train_message_list:\n",
    "        f.write(json.dumps(m)+'\\n')\n",
    "\n",
    "test_messages_list = []\n",
    "for idx in tqdm(range(len(test_set))):\n",
    "    input = test_set[idx][0]\n",
    "    target = test_set[idx][1]\n",
    "    prompt = \"You are a logging statement generator for Java. \" \\\n",
    "             \"You will be provided with a Java method as input. \" \\\n",
    "             \"Your task is to inject at least one logging statement at a rational position. \" \\\n",
    "             \"The output must be a completed Java method.\"\n",
    "    query = input \n",
    "    label = target\n",
    "    messages = {\"messages\": [{\"role\": \"system\", \"content\": prompt}, \n",
    "                             {\"role\": \"user\", \"content\": query}, \n",
    "                             {\"role\": \"assistant\", \"content\": label}]}\n",
    "\n",
    "    test_messages_list.append(messages)\n",
    "\n",
    "with open('./Data/test_data_codellama_it.jsonl', 'w') as f:\n",
    "    for m in test_messages_list:\n",
    "        f.write(json.dumps(m)+'\\n')\n",
    "\n",
    "T2 =time.perf_counter()\n",
    "print('Processing Time Total: %s s' % (T2 - T1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a84f78-71f1-4088-aefb-c28a9a2979f9",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21299ba1-ae95-4c5f-b673-22c5c0782bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    " \n",
    "import torch\n",
    " \n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM,\n",
    "                          TrainingArguments, Trainer, DataCollatorForSeq2Seq)\n",
    " \n",
    "# load customized dataset\n",
    "from datasets import load_dataset\n",
    " \n",
    "train_dataset = load_dataset('json', data_files='./Data/train_data_codellama_it.jsonl', split=\"train\")\n",
    "eval_dataset = load_dataset('json', data_files='./Data/test_data_codellama_it.jsonl', split=\"train\")\n",
    " \n",
    "# load base model\n",
    "base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481324c-1443-4db3-bf66-eaa6f29dc69e",
   "metadata": {},
   "source": [
    "# Fine-tuning with Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456a301-0a9a-40fc-930f-5e3ce1adf5ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train() # put model back into training mode\n",
    "model = prepare_model_for_int8_training(model)\n",
    " \n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\", \n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "model = get_peft_model(model, config)\n",
    " \n",
    "# keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    " \n",
    "batch_size = 8\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = batch_size // per_device_train_batch_size\n",
    "output_dir = \"code-llama-sft\"\n",
    " \n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        max_grad_norm = 0.3,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        weight_decay = 0.001,\n",
    "        warmup_ratio = 0.03,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-5,        \n",
    "        fp16=True,\n",
    "        logging_steps=500,\n",
    "        optim=\"adamw_torch\",\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        output_dir=output_dir,\n",
    "        load_best_model_at_end=False,\n",
    "        gradient_checkpointing=True,\n",
    "        report_to=\"none\", # if use_wandb else \"none\", wandb\n",
    "        run_name=f\"codellama-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\", # if use_wandb else None,\n",
    "        save_safetensors=False\n",
    "    )\n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    max_seq_length=2048,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd66fca-2c98-495d-aa61-cf61f2a76595",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012ae15-b704-4294-9d0f-724426761a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce58ac8-e89f-4825-9f72-299eb6fad9a9",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4eec52-b1aa-47b0-a7ea-4205e8056447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    " \n",
    "base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    " \n",
    "model_dir = './code-llama-sft/checkpoint-6811/'\n",
    "\n",
    "model = PeftModel.from_pretrained(model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e182b6-b123-45be-b2a5-986e2b764dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChatCompletion(prompt, content, model, tokenizer):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "         \"content\": content\n",
    "    }\n",
    "    ]\n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(tokenized_chat, max_new_tokens=512, pad_token_id=tokenizer.eos_token_id) \n",
    "    pred = tokenizer.decode(outputs[0][tokenized_chat.shape[1]:])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac85297-9472-42f0-978a-5e7dd569d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_list = []\n",
    "for idx in tqdm(range(len(test_set))):\n",
    "    input = test_set[idx][0]\n",
    "    target = test_set[idx][1]\n",
    "    prompt = \"You are a logging statement generator for Java. \" \\\n",
    "             \"You will be provided with a Java method as input. \" \\\n",
    "             \"Your task is to inject at least one logging statement at a rational position. \" \\\n",
    "             \"The output must be a completed Java method.\"\n",
    "    content = input\n",
    "    message = ChatCompletion(prompt, content, model, tokenizer)\n",
    "    messages_list.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da8aed-62c2-424a-9ef8-0da174639c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_dict = {'output': messages_list}\n",
    "output_pd = pd.DataFrame.from_dict(message_dict)\n",
    "output_pd.to_csv('./cllama_instruction-tuning_output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
