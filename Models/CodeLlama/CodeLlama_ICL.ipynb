{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97164e-bac4-4d71-9ff9-e50ffb879cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import random\n",
    "import jsonlines\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchsampler import ImbalancedDatasetSampler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b8cdb-847b-4792-ad0d-308f87e4ea08",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d4363-d388-4da5-8a36-e847143941ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogGenDataset(Dataset):\n",
    "    def __init__(self, tsv_dataset):\n",
    "        '''tsv dataset contain two columns. \n",
    "        First colomn includes the input.\n",
    "        Second column includes the target.'''\n",
    "\n",
    "        self.dataset = pd.read_csv(tsv_dataset, sep='\\t', header=None)\n",
    "\n",
    "        self.samples = []\n",
    "        for input, target in zip(self.dataset[0].tolist(), self.dataset[1].tolist()):\n",
    "            self.samples.append((input, target))\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.dataset[1].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[1].tolist())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.samples[idx][0]\n",
    "        target = self.samples[idx][1]\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a607c-dfe7-4ef8-a03c-dd077fe731d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = './Data/train_log4j.tsv'\n",
    "eval_df_path = './Data/eval_log4j.tsv'\n",
    "test_df_path = './Data/test_log4j.tsv'\n",
    "\n",
    "train_set = LogGenDataset(train_df_path)\n",
    "eval_set = LogGenDataset(eval_df_path)\n",
    "test_set = LogGenDataset(test_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c586c11f-8c11-4452-97a3-1a66558ab361",
   "metadata": {},
   "source": [
    "# Simple Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d27d5-2868-40ab-a93f-3bc937135a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChatCompletion(prompt, content, model, tokenizer):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "         \"content\": content\n",
    "    }\n",
    "    ]\n",
    "    \n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(tokenized_chat, max_new_tokens=512, pad_token_id=tokenizer.eos_token_id) \n",
    "    pred = tokenizer.decode(outputs[0][tokenized_chat.shape[1]:])\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896c4b5-771f-4a0f-ae01-3f0030533c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.float16).to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731efa1c-a24b-4c5b-9cec-fdfe1a8d2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_list = []\n",
    "for input, target in tqdm(test_set):\n",
    "    instruction = \"You will be provided with a Java method. Your task is to inject at least one logging statement at a \" \\\n",
    "                  \"rational logging point.\"\n",
    "    content = input\n",
    "    message = ChatCompletion(instruction, content, model, tokenizer)\n",
    "    messages_list.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc2550-3faa-476c-b230-2e169bb40ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_dict = {'output': messages_list}\n",
    "output_pd = pd.DataFrame.from_dict(message_dict)\n",
    "output_pd.to_csv('./cllama_simple-prompt_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df0b20-d3e2-4189-81c2-33dd6285b5a8",
   "metadata": {},
   "source": [
    "# Role Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9329c85-fa33-4dd0-a32c-9d07ee050326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChatCompletion(prompt, content, model, tokenizer):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "         \"content\": content\n",
    "    }\n",
    "    ]\n",
    "    \n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(tokenized_chat, max_new_tokens=512, pad_token_id=tokenizer.eos_token_id) \n",
    "    pred = tokenizer.decode(outputs[0][tokenized_chat.shape[1]:])\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e2f223-4f29-4963-af84-dbd22db02258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed065e2f977c496496eb34175eeec8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.float16).to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d372f-e01c-4b06-afab-545ccc603afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_list = []\n",
    "for input, target in tqdm(test_set):\n",
    "    role_prompt = \"You are a logging statement generator for Java. \" \\\n",
    "                  \"You will be provided with a Java method as input. \" \\\n",
    "                  \"Your task is to inject at least one logging statement at a rational position. \" \\\n",
    "                  \"The output must be a completed Java method.\"\n",
    "    content = input\n",
    "    message = ChatCompletion(role_prompt, content, model, tokenizer)\n",
    "    messages_list.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2a8c8-e214-4309-86cf-b76d809b1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_dict = {'output': messages_list}\n",
    "output_pd = pd.DataFrame.from_dict(message_dict)\n",
    "output_pd.to_csv('./cllama_role-prompt_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe57af-209f-4266-8caa-26940b516666",
   "metadata": {},
   "source": [
    "# Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c771b-8958-4082-b9d3-17ebe8258da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChatCompletion(prompt, content, model, tokenizer):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "         \"content\": content\n",
    "    }\n",
    "    ]\n",
    "    \n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(tokenized_chat, max_new_tokens=512, pad_token_id=tokenizer.eos_token_id) \n",
    "    pred = tokenizer.decode(outputs[0][tokenized_chat.shape[1]:])\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d3746-22b5-4bae-921e-68e6e21dc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.float16).to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b14a2b-b02b-42c1-8902-8d375d85ba83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages_list = []\n",
    "example_input = train_set[39358][0]\n",
    "example_output = train_set[39358][1]\n",
    "for input, target in tqdm(test_set):\n",
    "    instruction_prompt = \"Please Analyze the following provided code in Java. \" \\\n",
    "                        \"Generate at least one logging statement and inject it to the provided code. \" \\\n",
    "                        \"Logging statement is embedded in source code to understand system behavior, monitoring choke-points and debugging. \" \\\n",
    "                        \"A logging statement consist of logging level, logging message, and/or logging variable. \" \\\n",
    "                        \"The output must be a completed Java method. \\n\" \\\n",
    "                        \"Here are an example:\\n\" \\\n",
    "                        \"```The example input is\\n\" + example_input + \"\\n```\\n\" \\\n",
    "                        \"```The example output is\\n\" + example_output + \"\\n```\\n\" \\\n",
    "                        \"In this example, the generated logging statement for input is \" + '''\\'LOG . info ( \"Received node: \" + node . getIdentity ( ) + \" status: \" + node . getStatus ( ) + \" type: \" + node . getType ( ) ) ;\\'. ''' + \"The generated logging statement is injected in the FOR loop \\'for ( CssNode node : nodes ) { }\\'.\"\n",
    "    content = input\n",
    "    message = ChatCompletion(instruction_prompt, content, model, tokenizer)\n",
    "    messages_list.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826f4dd-b870-42d6-b3dc-cec2921967b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_dict = {'output': messages_list}\n",
    "output_pd = pd.DataFrame.from_dict(message_dict)\n",
    "output_pd.to_csv('./cllama_instruction-prompt_output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
